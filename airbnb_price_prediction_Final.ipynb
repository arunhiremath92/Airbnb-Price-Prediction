{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "df = pd.read_csv(\"src/listings_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source : https://www.kaggle.com/mhmdpkts/predict-price-ann-data-cleaning-future-engineering\n",
    "fig, ax = plt.subplots(figsize = (20, 5))\n",
    "sns.heatmap(df.isna(), cmap = \"cubehelix_r\", yticklabels='')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort out the Price-Section\n",
    "df.price = df.price.str.replace('$', '').str.replace(',', '').astype(float)\n",
    "df.cleaning_fee = df.cleaning_fee.str.replace('$', '').str.replace(',', '').astype(float)\n",
    "df.security_deposit = df.security_deposit.str.replace('$', '').str.replace(',', '').astype(float)\n",
    "df.extra_people = df.extra_people.str.replace('$', '').str.replace(',', '').astype(float)\n",
    "\n",
    "df['cleaning_fee'].fillna(0, inplace=True)\n",
    "df['security_deposit'].fillna(0, inplace=True)\n",
    "#  + df['cleaning_fee'] + df['security_deposit']\n",
    "df['total_price'] = df['price']\n",
    "df.drop(['price'], inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_checker(df):\n",
    "    df_nan = pd.DataFrame([[var, df[var].isna().sum() / df.shape[0], df[var].dtype]\n",
    "                           for var in df.columns if df[var].isna().sum() > 0],\n",
    "                          columns=['var', 'proportion', 'dtype'])\n",
    "    return df_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing 100% of the NULL values and identity values\n",
    "col_list_to_drop = [] \n",
    "columns = df.columns\n",
    "df_nan = nan_checker(df)\n",
    "length = len(df)\n",
    "for col in df.columns:  \n",
    "    if (df[col].dtype != 'float64'and df[col].nunique(dropna=True) == df[col].notnull().sum()):\n",
    "        col_list_to_drop.append(col)\n",
    "df.drop(col_list_to_drop, inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_nan.iterrows():\n",
    "    column_name = row[1]['var']\n",
    "    rate = round(row[1]['proportion'], 3)\n",
    "    if rate > 0.5:\n",
    "        df.drop(column_name, inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not need columns containing the following types of data-for our analysis\n",
    "\n",
    "\n",
    "columns with only true/false values\n",
    "columns that have text (not within the scope of our analysis)\n",
    "columns containing personnel information, except for the ones that guranntee some kind of assurance(licence, or profile pictures)\n",
    "\n",
    "Make everything as relevant, unless it is really irrelevant to your prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_host_text_columns =[\"scrape_id\",\n",
    "                           \"picture_url\",\n",
    "                           \"last_scraped\",\n",
    "                           \"name\",\n",
    "                           \"summary\", \n",
    "                           \"space\",\n",
    "                           \"description\",\n",
    "                           \"experiences_offered\", \n",
    "                           \"neighborhood_overview\",\n",
    "                           \"transit\",\n",
    "                           \"house_rules\", \n",
    "                           \"host_id\",\n",
    "                           \"host_url\",\n",
    "                           \"host_location\",\n",
    "                           \"host_neighbourhood\",\n",
    "                           \"host_name\",\n",
    "                           \"host_since\", \n",
    "                           \"host_thumbnail_url\",\n",
    "                           \"host_picture_url\",\n",
    "                           \"host_listings_count\",\n",
    "                           \"host_total_listings_count\", \n",
    "                           \"host_verifications\",\n",
    "                           \"calendar_updated\",\n",
    "                           \"has_availability\",\n",
    "                           \"availability_30\",\n",
    "                           \"availability_60\",\n",
    "                           \"availability_90\",\n",
    "                           \"calendar_last_scraped\",\n",
    "                           \"number_of_reviews\",\n",
    "                           \"first_review\",\n",
    "                           \"last_review\",\n",
    "                           \"review_scores_rating\",\n",
    "                           \"review_scores_accuracy\",\n",
    "                           \"review_scores_cleanliness\",\n",
    "                           \"review_scores_checkin\",\n",
    "                           \"review_scores_communication\",\n",
    "                           \"review_scores_location\",\n",
    "                           \"review_scores_value\",\n",
    "                           \"calculated_host_listings_count\", \n",
    "                           \"reviews_per_month\"]\n",
    "\n",
    "df_dropped = df.drop(review_host_text_columns, inplace=False, axis = 1).copy(deep=True)\n",
    "df_dropped.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Work on the Missing Data</h3>\n",
    "\n",
    " 1) Strategy 1: Drop all missing rows and check the outcome\n",
    " \n",
    " 2) Strategy 2: Impute the missing values with mean or most repetative element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strategy 1 \n",
    "df_dropped_rows = df_dropped.dropna()\n",
    "percentange_of_rows_dropped = (len(df_dropped) - len(df_dropped_rows))/len(df_dropped)\n",
    "print(\"Percentage of Rows Dropped  {}\".format(percentange_of_rows_dropped * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan = nan_checker(df_dropped_rows)\n",
    "df_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strategy 2 Imputing all the values\n",
    "from sklearn.impute import SimpleImputer\n",
    "#filling all the missing values with mean\n",
    "numerical_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "category__imputer = SimpleImputer(missing_values=np.nan,strategy=\"most_frequent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped_imputed = df_dropped_rows.copy(deep=True)\n",
    "df_nan = nan_checker(df_dropped_imputed)\n",
    "# df_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_nan.iterrows():\n",
    "    column_name = row[1]['var']\n",
    "    if df_dropped_imputed[column_name].dtype == 'float64':\n",
    "        df_dropped_imputed[column_name] = numerical_imputer.fit_transform(df_dropped_imputed[[column_name]])\n",
    "    else:\n",
    "        df_dropped_imputed[column_name] = category__imputer.fit_transform(df_dropped_imputed[[column_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan = nan_checker(df_dropped_imputed)\n",
    "df_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Null Data Removed for all columns </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decide between strategy 1 or 2\n",
    "df_na_filtered = df_dropped_imputed.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na_filtered.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping all information related to location, except for neighbour_group_cleansed: This column indicates which part of berlin the listings are located in. Secondly, the lattitude and logitude would be converted to distance from centre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_columns                 = [\"street\",\n",
    "                                   \"neighbourhood\",\n",
    "                                   \"neighbourhood_cleansed\",\n",
    "                                   \"neighbourhood_group_cleansed\",\n",
    "                                   \"city\",\n",
    "                                   \"state\",\n",
    "                                   \"zipcode\",\n",
    "                                   \"market\",\n",
    "                                   \"smart_location\",\n",
    "                                   \"country_code\",\n",
    "                                   \"country\",\n",
    "                                   \"is_location_exact\",\n",
    "                                   ]\n",
    "# for col in location_columns:\n",
    "#     print(\"------------------Column Name: {}\\n{}\".format(col, df_na_filtered[col].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location Columns having highly skewed values will be removed, todo: need a visual aid of all the columns and count of each value (except for latitude and longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na_filtered.drop(location_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the Lattitue and Logitude to convert to a distance from the city center,\n",
    "\n",
    "#source : https://www.kaggle.com/mhmdpkts/predict-price-ann-data-cleaning-future-engineering\n",
    "#create distance feature from latitude and longtitude \n",
    "from geopy.distance import great_circle\n",
    "def distance_from_berlin(lat, lon):\n",
    "    berlin_centre = (52.5027778, 13.404166666666667)\n",
    "    record = (lat, lon)\n",
    "    #returns surface distance in kilometers\n",
    "    return great_circle(berlin_centre, record).km\n",
    "\n",
    "#add distanse dataset\n",
    "df_na_filtered['distance'] = df_na_filtered.apply(lambda x: distance_from_berlin(x.latitude, x.longitude), axis=1)\n",
    "df_na_filtered.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
    "df_na_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the boolean to 0 or 1 value,\n",
    "boolean_mapping =  {\n",
    "    'True':1, 'False':0,\n",
    "    't':1, 'f':0, \n",
    "    'T':1, 'F':0, \n",
    "     0:0, 1:1\n",
    "}\n",
    "\n",
    "boolean_features = [\"host_is_superhost\",\n",
    "                    \"host_has_profile_pic\",\n",
    "                    \"host_identity_verified\",\n",
    "                    \"requires_license\",\n",
    "                    \"instant_bookable\",\n",
    "                    \"is_business_travel_ready\",\n",
    "                    \"require_guest_profile_picture\",\n",
    "                    \"require_guest_phone_verification\"]\n",
    "\n",
    "for col in boolean_features:\n",
    "    df_na_filtered[col]= df_na_filtered[col].map(boolean_mapping)\n",
    "    \n",
    "df_na_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting ammenities to a count\n",
    "#amenities count\n",
    "def amenities_counter(x):\n",
    "    return len(x.split(\",\"))\n",
    "\n",
    "\n",
    "df_na_filtered[\"amenities_count\"] = df_na_filtered.amenities.map(amenities_counter)\n",
    "df_na_filtered['Laptop_friendly_workspace'] = df_na_filtered['amenities'].str.contains('Laptop friendly workspace')\n",
    "df_na_filtered['TV'] = df_na_filtered['amenities'].str.contains('TV')\n",
    "df_na_filtered['Family_kid_friendly'] = df_na_filtered['amenities'].str.contains('Family/kid friendly')\n",
    "df_na_filtered['Host_greets_you'] = df_na_filtered['amenities'].str.contains('Host greets you')\n",
    "df_na_filtered['Smoking_allowed'] = df_na_filtered['amenities'].str.contains('Smoking allowed')\n",
    "df_na_filtered['Hot_water'] = df_na_filtered['amenities'].str.contains('Hot water')\n",
    "df_na_filtered['Fridge'] = df_na_filtered['amenities'].str.contains('Refrigerator')\n",
    "\n",
    "df_na_filtered['Laptop_friendly_workspace'] = df_na_filtered['Laptop_friendly_workspace'].map(boolean_mapping)\n",
    "df_na_filtered['TV'] = df_na_filtered['TV'].map(boolean_mapping)\n",
    "df_na_filtered['Family_kid_friendly'] = df_na_filtered['Family_kid_friendly'].map(boolean_mapping)\n",
    "df_na_filtered['Host_greets_you'] = df_na_filtered['Host_greets_you'].map(boolean_mapping)\n",
    "df_na_filtered['Smoking_allowed'] = df_na_filtered['Smoking_allowed'].map(boolean_mapping)\n",
    "df_na_filtered['Hot_water'] = df_na_filtered['Hot_water'].map(boolean_mapping)\n",
    "df_na_filtered['Fridge'] = df_na_filtered['Fridge'].map(boolean_mapping)\n",
    "\n",
    "amenities_columns = ['Laptop_friendly_workspace', 'TV', 'Family_kid_friendly', 'Host_greets_you', \n",
    "                     'Smoking_allowed', 'Hot_water', 'Fridge']\n",
    "\n",
    "\n",
    "\n",
    "#Drop the amenities column from prediction data-frame\n",
    "df_na_filtered.drop(['amenities'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose either to include individual Ammenities or just the count of ammenities in prediction, by concatinating the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reoriganize the property_type in to smaller set of types : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Property Types:\n",
    "property_mapping = {'Guesthouse':'Guesthouse', \n",
    "                    'Apartment':'Apartment', \n",
    "                    'Condominium':'Apartment', \n",
    "                    'Loft':'others',\n",
    "                    'House':'Apartment',\n",
    "                    'Serviced apartment':'Apartment',\n",
    "                    'Townhouse':'Apartment',\n",
    "                    'Boutique hotel':'Hotel',\n",
    "                    'Bed and breakfast':'Hotel',\n",
    "                    'Bungalow':'Apartment',\n",
    "                    'Boat':'others',\n",
    "                    'Guest suite':'Guesthouse',\n",
    "                    'Other':'others',\n",
    "                    'Island':'others',\n",
    "                    'Hostel':'others',\n",
    "                    'Train':'others',\n",
    "                    'Camper/RV':'others',\n",
    "                    'Houseboat':'others',\n",
    "                    'Cabin':'others',\n",
    "                    'Cottage':'Apartment',\n",
    "                    'Tiny house':'Apartment',\n",
    "                    'Chalet':'others',\n",
    "                    'Castle':'others',\n",
    "                    'Villa':'Apartment',\n",
    "                    'Aparthotel':'others',\n",
    "                    'Hotel':'Hotel',\n",
    "                    'Tipi':'others',\n",
    "                    'Resort':'Hotel',\n",
    "                    'In-law':'others',\n",
    "                    'Cave':'others',\n",
    "                    'Barn':'others',\n",
    "                    'Pension (South Korea)':'others',\n",
    "                    'Guesthouse':'Guesthouse',\n",
    "                    'Casa particular (Cuba)':'others',\n",
    "                    'others':'others'}\n",
    "df_na_filtered['property_type']=df_na_filtered['property_type'].map(property_mapping)\n",
    "# sns.set(style=\"darkgrid\")\n",
    "# fig, ax = plt.subplots()\n",
    "# sns.countplot(x=\"property_type\", data=df_na_filtered)\n",
    "# sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Trying the same with Labelencoder\n",
    "# from sklearn.preprocessing import LabelEncoder# creating initial dataframe\n",
    "# labelencoder = LabelEncoder()\n",
    "# df_na_filtered['property_type'] = labelencoder.fit_transform(df_na_filtered['property_type'])\n",
    "# df_na_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(style=\"darkgrid\")\n",
    "# fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(20, 5)\n",
    "# sns.countplot(x=\"neighbourhood_group_cleansed\", data=df_na_filtered)\n",
    "# sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "fig, ax = plt.subplots()\n",
    "sns.countplot(x=\"room_type\", data=df_na_filtered)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop or include the room_type\n",
    "#df_na_filtered.drop(['room_type'], inplace=True, axis=1)\n",
    "#df_na_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na_filtered[\"bed_type\"].unique()\n",
    "bed_mapping =  {'Real Bed':'Real Bed', \n",
    "                'Others':'Others',\n",
    "                'Pull-out Sofa': 'Others',\n",
    "                'Couch':'Others',\n",
    "                'Futon':'Others',\n",
    "                'Airbed':'Others'}\n",
    "df_na_filtered['bed_type']=df_na_filtered['bed_type'].map(bed_mapping)\n",
    "df_na_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na_filtered[\"cancellation_policy\"].unique()\n",
    "cancellation_mapping =  {'strict_14_with_grace_period':'strict', \n",
    "                'flexible':'flexible',\n",
    "                'moderate': 'moderate',\n",
    "                'super_strict_30':'strict',\n",
    "                'super_strict_60':'strict',\n",
    "                'strict':'strict'}\n",
    "df_na_filtered['cancellation_policy']=df_na_filtered['cancellation_policy'].map(cancellation_mapping)\n",
    "df_na_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a Categorical and Numerical Column List\n",
    "numerical_columns = []\n",
    "categoical_columns = []\n",
    "for column in df_na_filtered.columns:\n",
    "    if df_na_filtered[column].dtype == 'object':\n",
    "        categoical_columns.append(column)\n",
    "    else:\n",
    "        numerical_columns.append(column)\n",
    "\n",
    "data_frame_for_prediction = df_na_filtered.iloc[:, :].copy(deep=True)\n",
    "#You could choose to use either label encoder or one hot encoding\n",
    "data_frame_for_prediction = pd.get_dummies(data_frame_for_prediction, columns=categoical_columns)\n",
    "data_frame_for_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Engineering </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets start with prices\n",
    "red_square = dict(markerfacecolor='r', markeredgecolor='r', marker='.')\n",
    "data_frame_for_prediction['total_price'].plot(kind='box', xlim=(0, 1000), vert=False, flierprops=red_square, figsize=(16,2))\n",
    "df_prices = data_frame_for_prediction[ (data_frame_for_prediction.total_price > 1000) | (data_frame_for_prediction.total_price == 0) ]\n",
    "print(len(df_prices))\n",
    "print(data_frame_for_prediction['total_price'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will drop all the columns that are going to be beyond 2500 unit price range, this is an feature engineering test and can be optinally tested\n",
    "data_frame_for_prediction.drop(data_frame_for_prediction[ (data_frame_for_prediction.total_price > 1000) | (data_frame_for_prediction.total_price == 0) ].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "f, axarr = plt.subplots(3, 2, figsize=(10, 9))\n",
    "sns.scatterplot(x='host_is_superhost', y= 'total_price', data = data_frame_for_prediction,ax= axarr[0, 0])\n",
    "sns.scatterplot(x='host_identity_verified', y= 'total_price', data = data_frame_for_prediction,ax= axarr[0, 1])\n",
    "sns.scatterplot(x='requires_license', y= 'total_price', data = data_frame_for_prediction,ax= axarr[1, 0])\n",
    "sns.scatterplot(x='require_guest_profile_picture', y= 'total_price', data = data_frame_for_prediction,ax= axarr[1, 1])\n",
    "sns.scatterplot(x='require_guest_phone_verification', y= 'total_price', data = data_frame_for_prediction,ax= axarr[2, 0])\n",
    "sns.scatterplot(x='host_has_profile_pic', y= 'total_price', data = data_frame_for_prediction,ax= axarr[2, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_for_prediction.drop([\"host_is_superhost\",\n",
    "                                \"host_identity_verified\", \n",
    "                                \"require_guest_profile_picture\",\n",
    "                                \"require_guest_phone_verification\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the amenities and check the prediction\n",
    "df_without_ammenities = data_frame_for_prediction.drop(amenities_columns, inplace=False, axis=1).copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_ammenities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Ridge\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model Training and Validation using K-Fold Cross Validation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance(accuracy_model_train, accuracy_model_test, K):\n",
    "    avg_r = 0;\n",
    "    avg_rmse = 0;\n",
    "    print(\"\\nTraining:\")\n",
    "    for (r, rmse) in accuracy_model_train:\n",
    "        avg_r = avg_r + r\n",
    "        avg_rmse = avg_rmse + rmse\n",
    "    print(\"\\nR2 = {}  RMSE= {}\".format(round(avg_r/K, 3), round(avg_rmse/K, 3)))\n",
    "\n",
    "    avg_r = 0;\n",
    "    avg_rmse = 0;    \n",
    "    print(\"\\nTesting:\")    \n",
    "    for (r, rmse) in accuracy_model_test:\n",
    "        avg_r = avg_r + r;\n",
    "        avg_rmse = avg_rmse + rmse\n",
    "\n",
    "    print(\"\\nR2 = {}  RMSE= {}\".format(round(avg_r/K, 3), round(avg_rmse/K, 3)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-fold Validation :\n",
    "def train_model(model, b_transform):\n",
    "    df_kfold = data_frame_for_prediction.copy(deep=True)\n",
    "\n",
    "    #columns in prediction :\n",
    "    y = df_kfold['total_price'].copy(deep=True)\n",
    "    df_kfold.drop(['total_price'], inplace=True, axis=1)\n",
    "    X =  df_kfold\n",
    "\n",
    "    #scale all the values\n",
    "    sc = StandardScaler()\n",
    "    K = 10\n",
    "    kf = KFold(n_splits = K, random_state=None, shuffle=True)\n",
    "    iterationCount = 1\n",
    "    accuracy_model_train = []\n",
    "    accuracy_model_test = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "                print(\"\\n Fold \", iterationCount)\n",
    "                #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                X_train, X_test =  X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                if b_transform == 1:\n",
    "                    X_train = sc.fit_transform(X_train)\n",
    "                    X_test  = sc.fit_transform(X_test)\n",
    "                model.fit(X_train, y_train) \n",
    "\n",
    "                y_pred_train = model.predict(X_train)\n",
    "                r2_train = r2_score(y_train, y_pred_train)\n",
    "\n",
    "                y_pred_test = model.predict(X_test)\n",
    "                r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "                accuracy_model_train.append([r2_train, mean_squared_error(y_train, y_pred_train,  squared=False)])\n",
    "                accuracy_model_test.append([r2_test, mean_squared_error(y_test, y_pred_test,  squared=False)])\n",
    "                \n",
    "                iterationCount = iterationCount + 1   \n",
    "    print_performance(accuracy_model_train, accuracy_model_test, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n***********************************\\n\")\n",
    "print(\"\\nLinear Regression\\n\")\n",
    "model =  LinearRegression()\n",
    "train_model(model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n***********************************\\n\")\n",
    "print(\"\\nRidge Regression\\n\")\n",
    "model =  Ridge()\n",
    "train_model(model, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********************************\n",
      "\n",
      "\n",
      "Random Forest Regressor\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5d66a2007b15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n***********************************\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nRandom Forest Regressor\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n***********************************\\n\")\n",
    "print(\"\\nRandom Forest Regressor\\n\")\n",
    "model =  RandomForestRegressor(max_depth=5,verbose=1,random_state=0,n_estimators=200,n_jobs=-1)\n",
    "train_model(model, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n***********************************\\n\")\n",
    "print(\"\\nXG Boost - XGBRegressor\\n\")\n",
    "# instantiate xgboost with best parameters\n",
    "booster = xgb.XGBRegressor(colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, \n",
    "                           max_depth=6, n_estimators=200, random_state=4,silent=True)\n",
    "\n",
    "train_model(booster, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the Training Data:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#columns in prediction : \n",
    "columns_in_prediction = [col for col in data_frame_for_prediction.columns if col != \"total_price\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_frame_for_prediction[columns_in_prediction], data_frame_for_prediction['total_price'], test_size=0.2)\n",
    "\n",
    "#scale all the values\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test  = sc.transform(X_test)\n",
    "\n",
    "print(len(data_frame_for_prediction.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Fitting the model\n",
    "xgb_reg = xgb.XGBRegressor(silent=True)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "training_preds_xgb_reg = xgb_reg.predict(X_train)\n",
    "val_preds_xgb_reg = xgb_reg.predict(X_test)\n",
    "\n",
    "# Printing the results\n",
    "print(\"\\nTraining RMSE:\", round(mean_squared_error(y_train, training_preds_xgb_reg, squared=False),2))\n",
    "print(\"Validation RMSE:\", round(mean_squared_error(y_test, val_preds_xgb_reg, squared=False),2))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train, training_preds_xgb_reg),2))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_preds_xgb_reg),2))\n",
    "\n",
    "# Producing a dataframe of feature importances\n",
    "ft_weights_xgb_reg = pd.DataFrame(xgb_reg.feature_importances_, columns=['total_price'], index=columns_in_prediction)\n",
    "ft_weights_xgb_reg.sort_values('total_price', inplace=True)\n",
    "\n",
    "# Plotting feature importances\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.barh(ft_weights_xgb_reg.index, ft_weights_xgb_reg['total_price'], align='center') \n",
    "plt.title(\"Feature importances in the XGBoost model\", fontsize=14)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.margins(y=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Heavy Running Code </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "booster = xgb.XGBRegressor()\n",
    "# create Grid\n",
    "param_grid = {'n_estimators': [100, 150, 200],\n",
    "              'learning_rate': [0.01, 0.05, 0.1], \n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.6, 0.7, 1],\n",
    "              'gamma': [0.0, 0.1, 0.2]}\n",
    "\n",
    "# instantiate the tuned random forest\n",
    "booster_grid_search = GridSearchCV(booster, param_grid, cv=3, n_jobs=-1)\n",
    "\n",
    "# train the tuned random forest\n",
    "booster_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print best estimator parameters found during the grid search\n",
    "print(booster_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate xgboost with best parameters\n",
    "booster = xgb.XGBRegressor(colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, \n",
    "                           max_depth=6, n_estimators=200, random_state=4,slient=True)\n",
    "\n",
    "# train\n",
    "booster.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred_train = booster.predict(X_train)\n",
    "y_pred_test = booster.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(f\"r2: {round(r2, 4)}\")\n",
    "rmse = mean_squared_error(y_test, y_pred_test)\n",
    "print(f\"RMSE: {round(np.sqrt(rmse), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
